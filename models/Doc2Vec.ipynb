{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Modeling\n",
    "\n",
    "The code in the notebook was referenced from [this](https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4) Medium post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction import text \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "# modeling\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn import metrics, utils\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# metrics\n",
    "from sklearn import metrics, model_selection, svm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, plot_confusion_matrix, roc_curve, auc, classification_report\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing `clean_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.read_pickle(r'C:\\Users\\Ricky\\Desktop\\4.2 FINAL SEMESTER\\PROJECT II  Computer systems Project\\rOOT\\Preprocessing\\clean_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_votes</th>\n",
       "      <th>hate_speech_votes</th>\n",
       "      <th>other_votes</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>kindly say bickering to kikuyus and kalenjins....</td>\n",
       "      <td>kindly say bickering to kikuyus and kalenjins ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>kindly remind them that we do not have thoroug...</td>\n",
       "      <td>kindly remind them that we do not have thoroug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>kindly look at moses' statement. where has he ...</td>\n",
       "      <td>kindly look at moses statement where has he sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>kindly like this page&gt;&gt;&gt;wtf fun facts maasai a...</td>\n",
       "      <td>kindly like this pagewtf fun facts maasai are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>kindly kikuyus humble yourselves in 2022 and t...</td>\n",
       "      <td>kindly kikuyus humble yourselves in and take t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_votes  hate_speech_votes  other_votes  label  \\\n",
       "0            3                  1            3      0   \n",
       "1            3                  1            3      0   \n",
       "2            3                  1            3      0   \n",
       "3            3                  1            3      0   \n",
       "4            3                  1            3      0   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  kindly say bickering to kikuyus and kalenjins....   \n",
       "1  kindly remind them that we do not have thoroug...   \n",
       "2  kindly look at moses' statement. where has he ...   \n",
       "3  kindly like this page>>>wtf fun facts maasai a...   \n",
       "4  kindly kikuyus humble yourselves in 2022 and t...   \n",
       "\n",
       "                                        clean_tweets  \n",
       "0  kindly say bickering to kikuyus and kalenjins ...  \n",
       "1  kindly remind them that we do not have thoroug...  \n",
       "2  kindly look at moses statement where has he sa...  \n",
       "3  kindly like this pagewtf fun facts maasai are ...  \n",
       "4  kindly kikuyus humble yourselves in and take t...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_train, doc_test = train_test_split(clean_df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_train = doc_train.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['clean_tweets']), tags=[r.label]), axis=1)\n",
    "tagged_test = doc_test.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['clean_tweets']), tags=[r.label]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['fuck', 'you', 'zuck', 'you', 'canâ\\x80\\x99t', 'have', 'ai', 'constantly', 'police', 'â\\x80\\x9c', 'hatespeech', 'â\\x80', 'this', 'things', 'change', 'within', 'minutes', 'and', 'different', 'words', 'mean', 'different', 'things', 'in', 'different', 'pas', 'of', 'the', 'world', 'or', 'even', 'this', 'country'], tags=[0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_train.values[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training DBOW Model\n",
    "\n",
    "This is the Doc2Vec model analogous to Skip-gram model in Word2Vec. Here we can see that training a Doc2Vec model is much more straight forward in Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a doc2vec model, using only training data\n",
    "dbow_model = Doc2Vec(vector_size=100, \n",
    "                alpha=0.025, \n",
    "                min_count=5,\n",
    "                dm=1, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35122/35122 [00:00<00:00, 377857.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "# building vocabulary \n",
    "dbow_model.build_vocab([x for x in tqdm(tagged_train.values)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35122/35122 [00:00<00:00, 408589.21it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 675785.00it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 731990.45it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 494803.30it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 563385.48it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 605901.99it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 462257.89it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 557813.26it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 516765.75it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 616831.62it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 576084.69it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 702812.66it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 605884.55it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 517025.11it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 675434.87it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 494939.62it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 549093.10it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 373848.40it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 528075.05it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 717138.04it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 532484.41it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 747694.90it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 585641.83it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 627533.97it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 675410.10it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 347944.38it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 413441.03it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 408555.21it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 524562.44it/s]\n",
      "100%|██████████| 35122/35122 [00:00<00:00, 540691.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# this cell takes about 26 seconds to run\n",
    "for epoch in range(30):\n",
    "    dbow_model.train(utils.shuffle([x for x in tqdm(tagged_train.values)]), total_examples=len(tagged_train.values), epochs=1)\n",
    "    dbow_model.alpha -= 0.002\n",
    "    dbow_model.min_alpha = dbow_model.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the final vector feature for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#refer the next cell for the correct  output after the correct output after the elimination of #step=20 \n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words )) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "y_train, X_train = vec_for_learning(dbow_model, tagged_train)\n",
    "y_test, X_test = vec_for_learning(dbow_model, tagged_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logisitic Regression\n",
    "\n",
    "The Logisitic Regression baseline had the highest unweighted F1 score of 0.387805 with the Tf-IDF vectorization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.18 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ricky\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, n_jobs=1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_y_preds = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_precision = precision_score(y_test, logreg_y_preds)\n",
    "logreg_recall = recall_score(y_test, logreg_y_preds)\n",
    "logreg_f1_score = f1_score(y_test, logreg_y_preds)\n",
    "logreg_f1_weighted = f1_score(y_test, logreg_y_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "Weighted F1 Score: 0.906\n"
     ]
    }
   ],
   "source": [
    "print('Precision: {:.4}'.format(logreg_precision))\n",
    "print('Recall: {:.4}'.format(logreg_recall))\n",
    "print('F1 Score: {:.4}'.format(logreg_f1_score))\n",
    "print('Weighted F1 Score: {:.4}'.format(logreg_f1_weighted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like using Doc2Vec on a Logistic Regression model really lowers the F1 score, but it gets bumped up if we add the `weighted` parameter.\n",
    "\n",
    "Additionally, this method increased Precision but decreased Recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the scikit-learn documentation, a weighted F1 score calculates metrics for each label, and finds their average weighted by support (the number of true instances for each label). **This alters ‘macro’ to account for label imbalance;** it can result in an F-score that is not between precision and recall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary with all metrics\n",
    "metric_dict = {}\n",
    "metric_dict['Baseline Logisitic Regression'] = {'precision': logreg_precision, 'recall': logreg_recall, 'f1_score': logreg_f1_score, 'weighted_f1': logreg_f1_weighted}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "The baseline SVM had the highest weighted F1 score of 0.938102 with the Tf-IDF vectorization method.\n",
    "\n",
    "#########################to be edited and put on the corect F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_baseline = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(class_weight='balanced', gamma='auto', kernel='linear')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# this cell takes about 26 seconds to run\n",
    "# fit the training dataset on the classifier\n",
    "SVM_baseline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels on validation dataset\n",
    "SVM_y_preds = SVM_baseline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_precision = precision_score(y_test, SVM_y_preds)\n",
    "SVM_recall = recall_score(y_test, SVM_y_preds)\n",
    "SVM_f1_score = f1_score(y_test, SVM_y_preds)\n",
    "SVM_f1_weighted = f1_score(y_test, SVM_y_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Metrics for SVM Baseline with Lemmatization Features\n",
      "Precision: 0.1031\n",
      "Recall: 0.4748\n",
      "F1 Score: 0.1694\n",
      "Weighted F1 Score: 0.7798\n"
     ]
    }
   ],
   "source": [
    "# printing evaluation metrics up to 4th decimal place\n",
    "print('Testing Metrics for SVM Baseline with Lemmatization Features')\n",
    "print('Precision: {:.4}'.format(SVM_precision))\n",
    "print('Recall: {:.4}'.format(SVM_recall))\n",
    "print('F1 Score: {:.4}'.format(SVM_f1_score))\n",
    "print('Weighted F1 Score: {:.4}'.format(SVM_f1_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict['Baseline SVM'] = {'precision': SVM_precision, 'recall': SVM_recall, 'f1_score': SVM_f1_score, 'weighted_f1': SVM_f1_weighted}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>weighted_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline Logisitic Regression</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.906035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline SVM</th>\n",
       "      <td>0.103079</td>\n",
       "      <td>0.47479</td>\n",
       "      <td>0.169384</td>\n",
       "      <td>0.779814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               precision   recall  f1_score  weighted_f1\n",
       "Baseline Logisitic Regression   0.000000  0.00000  0.000000     0.906035\n",
       "Baseline SVM                    0.103079  0.47479  0.169384     0.779814"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(metric_dict, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, it looks like the SVM model does slightly better with unweighted F1, but the Logisitic Regression model does better with weighted F1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.94      1.00      0.97     14101\n",
      "     class 1       0.00      0.00      0.00       952\n",
      "\n",
      "    accuracy                           0.94     15053\n",
      "   macro avg       0.47      0.50      0.48     15053\n",
      "weighted avg       0.88      0.94      0.91     15053\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      0.72      0.82     14101\n",
      "     class 1       0.10      0.47      0.17       952\n",
      "\n",
      "    accuracy                           0.71     15053\n",
      "   macro avg       0.53      0.60      0.50     15053\n",
      "weighted avg       0.90      0.71      0.78     15053\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1']\n",
    "# logistic regression baseline\n",
    "print(classification_report(y_test, logreg_y_preds, target_names=target_names))\n",
    "# SVM baseline\n",
    "print(classification_report(y_test, SVM_y_preds, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it's important to note that the Doc2Vec method may be performing worse than the Tf-IDF method. We can try grid searching to improve this.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df5a47594a4c7fa77718041c239adbe925734d95d528854ea959dd8eef4ca91e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
